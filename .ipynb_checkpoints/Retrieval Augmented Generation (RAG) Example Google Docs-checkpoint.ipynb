{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a61ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eb672b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdab8cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "print(Path.cwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95240cdf",
   "metadata": {},
   "source": [
    "### We can now load those documents into memory with LangChain with 2 lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787824e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    '/Users/bytedance/Documents/jupyter_notebooks/google_reports/', # my local directory\n",
    "    #glob='.pdf',     # we only get pdfs\n",
    "    show_progress=True\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b557dab7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99265c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[2].page_content[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59ac3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9db9e78",
   "metadata": {},
   "source": [
    "### And we split them into chunks. Each chunk will correspond to an embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c1276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=0\n",
    ")\n",
    "docs_split = text_splitter.split_documents(docs)\n",
    "docs_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87239c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaa1f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_split[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed50979",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_split[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cde554b",
   "metadata": {},
   "source": [
    "### we will need to convert that data into embeddings and store those in a database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d012c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "2+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5de319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a31be5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars = os.environ.get()\n",
    "type(vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01baf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in os.environ.items():\n",
    "    print(i, j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbeb163",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key  = os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b34093",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(openai.api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73412a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.environ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1c7349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import pinecone \n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "# we use the openAI embedding model\n",
    "embeddings = OpenAIEmbeddings()\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,\n",
    "    environment=PINECONE_ENV\n",
    ")\n",
    "\n",
    "doc_db = Pinecone.from_documents(\n",
    "    docs_split, \n",
    "    embeddings, \n",
    "    index_name='langchain-demo'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8409e0ab",
   "metadata": {},
   "source": [
    "### We can now search for relevant documents in that database using the cosine similarity metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b84cf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Is tiktok taking share from youtube>?\"\n",
    "search_docs = doc_db.similarity_search(query)\n",
    "search_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd5e40d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b38becda",
   "metadata": {},
   "source": [
    "# Retrieving data with ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45720a5",
   "metadata": {},
   "source": [
    "### We can now use a LLM to utilize the database data. Let’s get a LLM. We could get GPT-3 using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d5f5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b463778",
   "metadata": {},
   "source": [
    "### or we could get ChatGPT using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2eb020a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822e0146",
   "metadata": {},
   "source": [
    "### Let’s use the RetrievalQA module to query that data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc91803",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type='stuff',\n",
    "    retriever=doc_db.as_retriever(),\n",
    ")\n",
    "\n",
    "query = \"What were the earnings in 2022?\"\n",
    "result = qa.run(query)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9587bda",
   "metadata": {},
   "source": [
    "### use agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0e533d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "# we load wikipedia\n",
    "tools = load_tools(['wikipedia'], llm=llm)\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools, \n",
    "    llm, \n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent.run('When was google created?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140e45db",
   "metadata": {},
   "source": [
    "### Another problem is that the LLM doesn’t remember what was said"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d887e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(\n",
    "    tools, \n",
    "    llm, \n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "agent.run('When was google created?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf6fd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run('By whom?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14b5baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key='chat_history')\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools, \n",
    "    llm, \n",
    "    agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION, \n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c24aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run('When was google created?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b964dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run('By whom?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec09028",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.agent.llm_chain.prompt.template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ac70b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895bfeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI()\n",
    "tools = load_tools([\n",
    "    'wikipedia', \n",
    "    'llm-math', \n",
    "    'google-search'\n",
    "], llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772a7146",
   "metadata": {},
   "source": [
    "### Utilizing the database as a tool\n",
    "We want to make sure ChatGPT can use the database alongside the other tools. We need to cast our RetrievalQA agent as a tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac5b10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "\n",
    "name = \"\"\"\n",
    "Alphabet quarterly earning reports database\n",
    "\"\"\"\n",
    "\n",
    "description = \"\"\"\n",
    "Useful for when you need to answer questions about the earnings of Google and Alphabet in 2021, 2022 and 2023. Input may be a partial or fully formed question.\n",
    "\"\"\"\n",
    "\n",
    "search_tool = Tool(\n",
    "    name=name,\n",
    "    func=qa.run,\n",
    "    description=description,\n",
    ")\n",
    "\n",
    "tools.append(search_tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91d50f1",
   "metadata": {},
   "source": [
    "### Solving a difficult problem: Should I invest in Google today?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c699db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.experimental.plan_and_execute import (\n",
    "    PlanAndExecute, \n",
    "    load_agent_executor, \n",
    "    load_chat_planner\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key='chat_history')\n",
    "planner = load_chat_planner(llm)\n",
    "executor = load_agent_executor(llm, tools, verbose=True)\n",
    "\n",
    "agent = PlanAndExecute(\n",
    "    planner=planner, \n",
    "    executor=executor, \n",
    "    verbose=True, \n",
    "    reduce_k_below_max_tokens=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca93b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run('Should I invest in Google now?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c5ddf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f936504b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d1ceb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9756aa65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352def63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282cd884",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c41015f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef5a4a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
